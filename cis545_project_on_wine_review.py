# -*- coding: utf-8 -*-
"""CIS545_Project_On_Wine_Review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1blPhxx6h7tPPyNpz1XRfzrl8rFDjH-nq

# **CIS5450 Big Data Analystics on Wine Review**

<div class = "row">
  <p class="d-flex" align = "left">
    <img src="https://cdn.pixabay.com/photo/2014/12/02/03/11/grapes-553462_1280.jpg" width="520px" height="320px">
  </p>
  <h2 class="column">
The wine industry is an extensive and complex ecosystem with thousands of wine varietals, production methods, and regions. The use of big data analytics in the wine industry has the potential to unlock invaluable insights for wine growers, winemakers, and consumers alike. In this course, we propose to explore the use of big data analytics and build a pricing model which can be applied for predicting a specific wine price by given information. This model will be valuable for market positioning and pricing strategy in the wine industry. 

  </h2>
  <div class="column">
  </div>
</div>

### This project is divided into three main parts.

**Part I**: Data analysis and preprocessing, transform raw data into correct format for following model construction.

**Part II**: Model Selection, training and tuning for two types of Regression model: Linear Regression and Logistic Regression.

**Part III**: Expand research with more powerful machine learning modes: Decision Tree, Random Forest and Neural Network Model.

# PART 1: Data Wrangling, Transforming, Information Extraction

Data preprocessing is a crucial step in this research. After cleaning raw data, we explored the data relevance and defined the hypothesis to this research. After that,  transformed different characteristics data into informative features for Machine Learning Model.

## 1.1 Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# Data Wrangling
import pandas as pd
import numpy as np
import re

#nltk
import nltk
from nltk.corpus import stopwords
from nltk.sentiment import SentimentIntensityAnalyzer

#Counter
from collections import Counter

#Plotting
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud
# from IPython.display import Image
# import pydotplus
# %matplotlib inline

# Run this cell to mount the drive (will be prompted to sign in)
from google.colab import drive
drive.mount('/content/drive')

"""## 1.2 Load Data from Kaggle




"""

!pip install kaggle

# Create the kaggle directory and read the uploaded kaggle.json file
# (NOTE: Do NOT run this cell more than once unless restarting kernel)
!mkdir ~/.kaggle

# Read the uploaded kaggle.json file
!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/

# Download dataset
!!kaggle datasets download -d zynicide/wine-reviews

!unzip /content/wine-reviews.zip

"""The data from Kaggle contains two .csv files. We read the .csv files and concat them to a dataframe called `raw_wine_data`."""

# Read the csv files and concat them to a dataframe called `raw_wine_data`
raw_wine_data1 = pd.read_csv('winemag-data_first150k.csv')
raw_wine_data2 = pd.read_csv('winemag-data-130k-v2.csv')
# print(len(raw_wine_data1))
# print(len(raw_wine_data2))
raw_wine_data = pd.concat([raw_wine_data1, raw_wine_data2])
# Check out the first five rows
raw_wine_data = raw_wine_data.reset_index(drop=True)
raw_wine_data.head(5)

"""## 1.3 Wine Dataset EDA

In this following part, we are going to explore information step by step and represent how to extract meaningful features from given dataset. Accordingly, We proposed a hypothesis that there are five key factors having influence on wine price.


*   Country
*   Variety
*   Year
*   Review Point
*   Winery

### 1.3.1 Checking Basic Information

DataFrame Data Conclusion
Data size conclusion: there are total 280901 entries in the yelp business dataframe.

We first set a clolumn as "id" for each wine record, as the data has no unique feature that can represent as id for each record.
"""

raw_wine_data.info()

# update index for each individual record
raw_wine_data = raw_wine_data.reset_index()
raw_wine_data

"""### 1.3.2 Relationship between Winery and Region

We want to decide whether to keep region_1 or winery as a feature. We want to find if winery can directly represent region_1 as ONE feature, or vice versa. So we count how many kinds of wine were produced by a region-winery pair, as well as how many wineries are associated with each region. 

After doing that, we find that there are 16573 wineries that have only 1 region, and there are 1332 regions that have 5 wineries. We also find that 75% of the wineries have only 1 or 2 regions. So we decide to keep winery as a feature instead of region_1.

### Distribution 1: How Many Species of Wines does a Winery-Region pair Cooperate?

To have an idea about the relationship between the region and winery, we count how may kinds of wine were produced by a region-winery pair.
"""

region_winery = raw_wine_data[['region_1', 'winery']]
region_winery['count'] = pd.Series([0 for i in range(len(region_winery.index))])
region_winery_count = region_winery.groupby(['region_1','winery']).count().reset_index()

region_winery_count.describe()

region_winery_count['count'].plot(kind='kde', xlim=(0,30))

"""### Distribution 2: How Many Wineries does a Region Sell Grapes to?

We count how many wineries are associated with each region
"""

wineries_per_region = region_winery_count[['region_1', 'winery']].groupby('region_1').count().reset_index()
wineries_per_region.describe()

wineries_per_region['winery'].plot(kind='kde', xlim=(0,30))

"""### Distribution 3: How Many Regions does a Winery Purchase Grapes from?

We count how many regions are associated with each winery
"""

regions_per_winery = region_winery_count[['winery', 'region_1']].groupby('winery').count().reset_index()
regions_per_winery.describe()

regions_per_winery['region_1'].plot(kind='kde', xlim=(0,10))

"""### 1.3.3 Drop Less Valuable Columns

We drop the following 7 columns:Unnamed: 0','designation','province','region_1','region_2','taster_twitter_handle','taster_name', as they are irrelevant to our price prediction.'

*   Unnamed: replaced it by column 'id' to represent each unique rocord
*   designation: highly related with winery, we keep winery for analysis
*   Province: this analysis is not going to have any depth into specific country
*   region_1: (refer to 1.3.1 for more details)
*   region_2:  keep country as location information
*   taster_twitter_handle: not relevant to all features and wine price
*   taster_name: not relevant to all features and wine price
"""

raw_wine_data_dropCol = raw_wine_data.drop(columns=['Unnamed: 0','designation','province','region_1','region_2','taster_twitter_handle','taster_name'])
raw_wine_data_dropCol.rename(columns={'index':'id'}, inplace=True)

raw_wine_data_dropCol

"""### 1.3.4 Drop Null Tuples

*  Check on the non-null count of each attributes, and find out that all of them (except "title") have high percentage of non-null values. 

*   Drop null data in the column which is less valuable and has less impact on the data size. 
*   To avoid dropping too many training data, keep the nulls in column 'title', and conduct the data imputation in the following part.
"""

raw_wine_data_dropCol.info()

# keep the null data in "title"
raw_wine_data_dropCol_dropNa = raw_wine_data_dropCol.dropna(subset=['id', 'country', 'description', 'points', 'price', 'variety', 'winery'])
raw_wine_data_dropCol_dropNa.info()

"""### 1.3.5 Keep Data for Top 20 Countries

"Country" is a feature that we want to use to predict price. As the quality of a wine may be affected by the climate of a particular region that grows grapes, and also by the development of that country(economic, transportation, international relations, etc).

As there are many countries from the data. We want to reduce the complexity while keep adequate amount and balance of the data. So we decide to reduce the countries to only the top 20 countries that are most poplular to tasters.
"""

# country_top20 list
country_top20 = raw_wine_data_dropCol_dropNa.groupby('country').count().sort_values('title',ascending=False)[0:20].reset_index()['country']
country_top20_list = country_top20.tolist()
country_top20_list

# get the data from top 20 countries
wine_data_top20countries = raw_wine_data_dropCol_dropNa[raw_wine_data_dropCol_dropNa['country'].isin(country_top20_list)]

entry_country_df = wine_data_top20countries[['id','country']].groupby(['country']).count().reset_index()
entry_country_df.rename(columns={'id':'entry count'}, inplace=True)
sns.set(rc={"figure.figsize":(15, 6)}, style="whitegrid")
g3 = sns.catplot(data=entry_country_df, x='country', y='entry count', height=8, aspect=1.5, kind='bar', dodge=False, log=True)
g3.set_xticklabels(rotation=90)
g3.set(title = 'entry count by country')

"""### 1.3.6 Keep Data for Top 20 Varieties

Variety is the variety of the grape that is used to make the wine. We would like "Variety" to be one feature for the price prediction. 

As there are many varieties from the data. We want to reduce the complexity while keep adequate amount and balance of the data. So we decide to reduce the varieties to only the top 20 varieties that are most popular to make the wine in the data.
"""

# variety_top20 list
variety_top20 = wine_data_top20countries.groupby('variety').count().sort_values('title',ascending=False)[0:20].reset_index()['variety']
variety_top20_list = variety_top20.tolist()

variety_top20_list

wine_data_top20 = wine_data_top20countries[wine_data_top20countries['variety'].isin(variety_top20_list)]
wine_data_top20.head(5)

"""### 1.3.7 Extract "year" information from "title"

Title contains information about winery, year, province, and variety about the wine.

Since "winery", "province", and "variety" have their own attributes, all we need is the "year" information, which can be extracted by regular expression as follow.
"""

def extract_year(title):
    if title == np.nan: return np.nan
    digits = re.findall(r'\d{4}', title)
    if len(digits)>0: return digits[0]
    return np.nan

wine_data_top20  = wine_data_top20.astype({'title':'str'})
wine_data_top20['year'] = wine_data_top20['title'].apply(extract_year)
wine_data_top20['year'] = wine_data_top20['year'].astype('Int64')
wine_data_top20.drop(columns=['title'], inplace=True)

"""### 1.3.8 outliear treatment

It is noticed that some "year" information has fallen out the reasonable range, such as 1000 and 7200, for marketing purpose. 

We inspected the data distribution as follow, then find out the "off" years (106 tuples)  that are not in the range of 1980 to 2022 and set them to be null.
"""

wine_data_top20.describe()

wine_data_top20[(wine_data_top20['year']<1980)|(wine_data_top20['year']>2022)]

wine_data_top20['year'] = wine_data_top20['year'].astype('str')
wine_data_top20['year'] = wine_data_top20['year'].apply(lambda x:x if '1980'<x<'2022' else np.nan)
wine_data_top20['year'] = wine_data_top20['year'].astype('Int64')

wine_data_top20.describe()

wine_data_top20.info()

"""### 1.3.9 Add a Boolean Feature for the missing "year"

As we can see in the above cell, there are only 85676 tuples with non-null "year" value out of 188595 total tuples.

It is attempting to use the simple imputing, however, due to the unknown correlation with other features and potential bias, we determined to add a boolean featrue for the missing "year". As the lecture described, this method is analogous to "one-hot encoding" and should be a better choice to solve this issue.
"""

wine_data_top20.fillna(value={'year':0}, inplace=True)
wine_data_top20['missing_year']=wine_data_top20['year'].apply(lambda x:1 if x==0 else 0)

wine_data_top20.head(5)

wine_data_top20.info()

"""##  1.4 Natural Language Processing (NLP)

The attribute of "description" contains detailed information about the quality, color, scent, and flavor from the testers, which makes it a perfect chance to apply the NLP techniques to process for tags and extract sentiment.

At the end of this section, we expect the have a numerical attribute of "sentiment" as a feature to the dataset, and plot the word cloud to see the frequently used words in description.

### 1.4.1 Download Packages from Natural Language Toolkit (nltk)
"""

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('vader_lexicon')
nltk.download('averaged_perceptron_tagger')
stopwords = set(stopwords.words('english'))

"""### 1.4.2 Extract "description" and tokenize

At the first step of the NLP, we will tokenize the description into a clean format.
"""

# TO-DO: extract description	
description_df = wine_data_top20[['id', 'description']]
description_df['description'] = description_df.apply(lambda x: str(x['description']), axis=1)
description_df

# Convert to dataframe & tokenize list 
# return a tokenized list of words, removing stop words and checking for alphabets
# in description_df, create a new column called 'tokenized' which contains the tokens from description.

from nltk.tokenize import word_tokenize 
def tokenize_content(content):
  word_list = word_tokenize(content)
  word_list = list(map(lambda x: x.lower(), word_list))
  # print(word_list)
  word_list_final = []
  count = 0;
  # print(len(word_list))
  while count < len(word_list):
    # print(count)
    word = word_list[count]
    # print(word_list[count])
    if word in stopwords:
      count = count + 1
      continue
    if word.isalpha() == True:
      word_list_final.append(word)
    count = count + 1
  # print(word_list_final)
  return word_list_final

  
#TODO: Apply the function to description
description_df['tokenized'] = description_df.apply(lambda x: tokenize_content(x['description']), axis=1)
description_df.info()

"""### 1.4.3 Create pos tag from the tokens

After tokenizing the descriptions, we create pos tag for each tokens and classify each vocabulary.
"""

# nltk pos tag the tokenized words
# from nltk import pos_tag
description_df['tagged'] = description_df.apply(lambda x: nltk.pos_tag(x['tokenized']), axis=1)
description_df.head(5)

"""### 1.4.4 Sentiment analysis

After tokenizing and tagging, we caculate the sentiment score. We save it as a column in our main dataframe. As sentiment score represents the positivity/negativity of the taster, we decide to have "sentiment" as a feature of our prediction.
"""

# Sentiment analysis
sia = SentimentIntensityAnalyzer()
def retrieve_sentiment(content):
  return sia.polarity_scores(content)['compound']

description_df['sentiment'] = description_df.apply(lambda x: retrieve_sentiment(x['description']), axis=1)
description_df.head(5)

"""We add "sentiment" into wine_df."""

wine_df = wine_data_top20[['id', 'country', 'points', 'price', 'variety', 'year', 'missing_year', 'winery']].merge(description_df[['id','sentiment']], on='id',how='left')

wine_df.head(5)

wine_df.info()

"""### 1.4.5 Generate Word Cloud

We select adjective words (tagged as "JJ") from the tagged words. From these adjective words (as a non-repeated set) we generate Word Cloud.
"""

# # select adjective words for prediction

description_df['adjectives'] = description_df.apply(lambda x: set([word for word,tag in x['tagged'] if tag[0] in "JJ"]), axis=1)
description_df.head(5)

# from textblob import TextBlob
# def get_adjectives(text):
#     blob = TextBlob(str(text))
#     return set([ word for (word,tag) in blob.tags if tag == "JJ"])

# description_df['adjectives'] = description_df['description'].apply(get_adjectives)
# description_df

# TODO: flatten adjectives set
adjective_set_list = description_df["adjectives"].values.tolist()
adjective_list = []
for adjective_subset in adjective_set_list:
  for adj in adjective_subset:
    adjective_list.append(adj)
# adjective_list

#  Save the result as a list of (word, count) tuples, in descending order of count.
cnt = Counter()
for word in adjective_list:
  cnt[word] += 1

word_frequency_list = cnt.most_common()
# word_frequency_list

# generate wordCloud
wordcloud = WordCloud(background_color ='white').generate_from_frequencies(cnt)

# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
plt.show()

"""##1.5 Data Preposessing and Visualization

### 1.5.1 Preparing Rating Score for Winery

Originally we would like "winery" to be one of the features. But there are too many wineries to do one hot encoding. And there will not be enough data if we only keep a few wineries (say, top 20). So we calculate the average points for each winery. We decide to use "winery_point" as one feature to represent the quality/reputation of a winery.
"""

winery_points = wine_df[['winery','points']].groupby('winery').mean().round(2).reset_index()
winery_points.rename(columns={'points':'winery_point'},inplace=True)

winery_points.head(5)

"""add winery point as a new column """

# Merge with wine_df, add one more column winery_point
wine_df = wine_df.merge(winery_points, how="left", on='winery')
wine_df.drop(columns=['winery'],inplace=True)

wine_df.head(5)

"""### 1.5.2 average price trend by year (20 varieties)

We suspect that "year" can affect the price of a wine (the older the year is, the more expensive a wine is). Our hypothesis is verified after we line-plot the average price trend of various types of wine by year. So we would like "year" to be one feature of our prediction.
"""

price_year_variety_df = wine_df[['price', 'variety', 'year']]
price_year_variety_df = price_year_variety_df[price_year_variety_df['year']>1980]
price_year_variety_df = price_year_variety_df[price_year_variety_df['variety'].isin(['Pinot Noir','Chardonnay','Cabernet Sauvignon','Riesling','Sauvignon Blanc','Syrah','Merlot','Zinfandel','Malbec'])]
price_year_variety_df['year']=price_year_variety_df['year'].astype('int64')
price_year_variety_df = price_year_variety_df.groupby(['variety', 'year']).mean().reset_index()
price_year_variety_df.rename(columns={'price':'average price','year':'production year'},inplace=True)

sns.set(rc={"figure.figsize":(15, 6)}, style="whitegrid")
g1 = sns.lineplot(data=price_year_variety_df, x='production year', y='average price', hue='variety')
g1.legend(fontsize=8)
g1.set(title = 'average price trend by production year')

"""### 1.5.3 violinplot price by variety

We check different price distribution for variety and find that the price for different variety varies. So we would like to have "variety" to be a feature for our prediction.
"""

price_variety_df = wine_df[['price', 'variety']]
sns.set(rc={"figure.figsize":(15, 6)}, style="whitegrid")
g2 = sns.catplot(data=price_variety_df, x='variety', y='price', hue='variety', height=8, aspect=1.5, kind='violin', dodge=False)
g2.set_xticklabels(rotation=90)
g2.set(title = 'violinplot price by variety')

"""### 1.5.4 entry count by variety

From the pie chart we find that the variety is distributed relatively evenly, which means the data is relatively balanced. So we decide to have "variety" as a feature for our prediction.
"""

entry_variety_df = wine_df[['id','variety']]
entry_variety_df.rename(columns={'id':'entry count'}, inplace=True)
entry_variety_df.groupby(['variety']).count().plot(kind='pie', y='entry count', ylabel='', autopct='%1.0f%%', figsize=(18,9), legend=False)

"""### 1.5.5 sentiment by country

Sentiment can to some degree represent the quality of a wine. We want to get a broader picture of the wine quality in various countries.
"""

sentiment_country_df = wine_df[['sentiment','country']]
sns.set(rc={"figure.figsize":(15, 6)}, style="whitegrid")
g2 = sns.catplot(data=sentiment_country_df, x='country', y='sentiment', hue='country', height=8, aspect=1.5, kind='box', dodge=False)
g2.set_xticklabels(rotation=90)
g2.set(title = 'boxplot sentiment by variety')

sentiment_country_df = wine_df[['sentiment','country']].groupby(['country']).mean().reset_index()
sentiment_country_df.rename(columns={'sentiment':'average sentiment'}, inplace=True)
sns.set(rc={"figure.figsize":(15, 6)}, style="whitegrid")
g3 = sns.catplot(data=sentiment_country_df, x='country', y='average sentiment', height=8, aspect=1.5, kind='bar', dodge=False)
g3.set_xticklabels(rotation=90)
g3.set(title = 'average sentiment by country')

"""### 1.5.6 general price distribution

We check the price distribution. This can help us categorize the price in Part 2.
"""

wine_df['price'].plot(kind='kde', xlim=(0,300))

"""Temporarily save the intermediate data for later use."""

wine_df.to_csv('/content/drive/MyDrive/wine_clean.csv', index=False)

"""# PART 2: Regression Model Evaluation and Hyperparameter Tuning

In this part, we are going to train regression model and solve the Multicollinearity problem.


*   Feature preparing for ML model
*   Multicollinearity and PCA
*   Linear Regression model 
*   Logistic Regression model

## 2.1 Feature Engineering

According to our Hypothsis and given dataset, there are several data tranformation work required before training a ML model. 


*   One Hot Encoding on Categorical Features
*   Categorize the price as Label
*   Balance the price label in given data sample

### 2.1.1 Import Libraries
"""

# Data Wrangling
import pandas as pd
import numpy as np

# drive mount
from google.colab import drive
drive.mount('/content/drive')

# PCA
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

#Regression 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import r2_score

#Cross Validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold

#Model tuning
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso

#Metrics
from sklearn.metrics import r2_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score

"""### 2.1.2 load data and backup"""

# download clean data for modeling 
wine_df  = pd.read_csv('/content/drive/MyDrive/wine_clean.csv')

wine_df

"""### 2.1.3 One Hot Encoding on categorical features of "country" and "variety"

"""

# TO-DO: One-hot encode on imputed wine_df
wine_df  = pd.get_dummies(data=wine_df, columns=['country','variety'])
wine_df.head(5)

"""### 2.1.4 categorize the price into labels of "wine class"
We refer to the following information for the categorization based on wine price.
<p class="d-flex" align = "left">
    <img src="https://drive.google.com/uc?export=view&id=1VfwQL7gy-JorgAJuNxUx3clz3OhOKZQW" width="320px" height="240px">
</p>
<p>But after we try to categorize our data as above, we find imbalance in data. So after adjustment, we define 8 different classification based on wine price as follows:</p>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-7btt{border-color:inherit;font-weight:bold;text-align:center;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-7btt">label</th>
    <th class="tg-7btt">wine class</th>
    <th class="tg-7btt">price</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-c3ow">0</td>
    <td class="tg-c3ow">value</td>
    <td class="tg-c3ow">0 - 10</td>
  </tr>
  <tr>
    <td class="tg-c3ow">1</td>
    <td class="tg-c3ow">popular</td>
    <td class="tg-c3ow">11 - 15</td>
  </tr>
  <tr>
    <td class="tg-c3ow">2</td>
    <td class="tg-c3ow">premium</td>
    <td class="tg-c3ow">16 - 20</td>
  </tr>
  <tr>
    <td class="tg-c3ow">3</td>
    <td class="tg-c3ow">super premium</td>
    <td class="tg-c3ow">21 - 30</td>
  </tr>
  <tr>
    <td class="tg-c3ow">4</td>
    <td class="tg-c3ow">ultra premium</td>
    <td class="tg-c3ow">31 - 50</td>
  </tr>
  <tr>
    <td class="tg-c3ow">5</td>
    <td class="tg-c3ow">luxury</td>
    <td class="tg-c3ow">51 - 100</td>
  </tr>
  <tr>
    <td class="tg-c3ow">6</td>
    <td class="tg-c3ow">super luxury</td>
    <td class="tg-c3ow">101 - 200</td>
  </tr>
  <tr>
    <td class="tg-c3ow">7</td>
    <td class="tg-c3ow">icon</td>
    <td class="tg-c3ow">over 201</td>
  </tr>
</tbody>
</table>

Original categorization for wine price.
"""

def wineClass(x):
    if x <= 10: return 0
    if 10 < x <= 15: return 1
    if 15 < x <= 20: return 2
    if 20 < x <= 30: return 3
    if 30 < x <= 50: return 4
    if 50 < x <= 100: return 5
    if 100 < x <= 200: return 6
    return 7

"""But this categorization leads to imbalance of data."""

wine_df['wine_class'] = wine_df['price'].apply(wineClass)
wine_class_count = wine_df[['wine_class','id']].groupby('wine_class').count().reset_index()
wine_class_count.rename(columns={'id':'count'},inplace=True)
sns.set(style="whitegrid")
g4 = sns.catplot(data=wine_class_count, x='wine_class', y='count', height=8, aspect=1.5, kind='bar', dodge=False)
g4.set(title = 'review count by wine class')

"""### 2.1.5 balance the labels of "wine class" by adjusting the price range

We evenly partition the data into 8 parts and check the price of each partition.
"""

price_sorted_df = wine_df[['price']].sort_values('price')
line_number = len(price_sorted_df.index) // 8
indices_selected = [line_number*m for m in range(1,8)]
price_sorted_df.iloc[indices_selected]

"""We categorize the price based on above partition."""

def balancedClass(x):
    if x <= 13: return 0
    if 13 < x <= 17: return 1
    if 17 < x <= 20: return 2
    if 20 < x <= 26: return 3
    if 26 < x <= 34: return 4
    if 34 < x <= 44: return 5
    if 44 < x <= 60: return 6
    return 7

"""We check the distribution again but with the new categorization. This time we find the data is balanced for each category."""

wine_df['wine_class'] = wine_df['price'].apply(balancedClass)
wine_class_count = wine_df[['wine_class','id']].groupby('wine_class').count().reset_index()
wine_class_count.rename(columns={'id':'count'},inplace=True)
sns.set(style="whitegrid")
g4 = sns.catplot(data=wine_class_count, x='wine_class', y='count', height=8, aspect=1.5, kind='bar', dodge=False)
g4.set(title = 'review count by wine class')

"""##2.2 Correlation Analysis

Correlation Analysis helps us to understand the multicollinearity problems in our dataset. 

Since there are lots of correlation in features, we try to use PCA to remove the correlations and decrease the dimension in the Model. 
"""

numeric_df = wine_df
numeric_df = numeric_df[numeric_df['year']>0]
corr_matrix = numeric_df.corr()
sns.set(rc={"figure.figsize":(8, 8)})
ax = sns.heatmap(corr_matrix, cmap = 'RdYlBu_r', vmin = -1, vmax = 1)
ax.set(title='correlation heatmap')
plt.show()

"""## 2.3 Linear Regression Model

In this part, we focus on Linear Regression model and two modifications:
* Training Linear Regression model by original training data
* Training Linear Regression model by PCA 
* Regulazation Model: Lasso vs. Ridge Model

By comparing the Accuracies between different model, find a best model.

###2.3.1 Linear Regresion
"""

#split train and test dataset
# Preparing data for train set and test set
X = wine_df.drop(columns=['id', 'price', 'wine_class'])
y = wine_df['wine_class']
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2)

# step 1: Trainning model by using all variables
clf = LinearRegression()
clf.fit(X_train1,y_train1)

# step 2. Testing model 
y_pred1 = clf.predict(X_test1)

# step 3: evaluate model 
score = clf.score(X_test1, y_test1)
score

"""###2.3.2 Apply PCA in Linear Regression Model

We can observe that the accuracy is kept almost at the same level while the dimensionality of the data has been largely reduced.

"""

# PCA 

# Preparing data for train set and test set
X = wine_df.drop(columns=['id', 'price', 'wine_class'])
y = wine_df['wine_class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Intermediate step to address scale-invariance
x_train_scale =  StandardScaler().fit_transform(X_train)
x_test_scale =  StandardScaler().fit_transform(X_test)

# Instantiate and Fit PCA
pca = PCA(n_components = 45)
x_train_pca = pca.fit_transform(x_train_scale)
x_test_pca = pca.transform(x_test_scale)

#  Save the explained variance ratios into variable called "explained_variance_ratios"
np.set_printoptions(suppress=True)
explained_variance_ratios = pca.explained_variance_ratio_

# Save the CUMULATIVE explained variance ratios into variable called "cum_evr"
cum_evr = np.cumsum(explained_variance_ratios)

# Plot
plt.figure(figsize=(12,6))
plt.plot(cum_evr)
# Aesthetics 
plt.title('PCA Analysis')
plt.xlabel('Components') 
plt.ylabel('Explained Variance Ratio')
plt.axhline(0.8)
plt.xlim(1,46)  # x range
x=range(1,46,1) 
plt.xticks(x)    # setting for xticks
plt.show()

# Final PCA: reduce dimensionality to 28 components

# 1: Preparing data for train set and test set
X = wine_df.drop(columns=['id', 'price', 'wine_class'])
y = wine_df['wine_class']

# 2. scale training and testing data
scaler= StandardScaler().fit(X)
X_scale = scaler.transform(X)
X_train_scale, X_test_scale, y_train, y_test = train_test_split(X_scale, y, test_size=0.2, random_state=42)

# 3. transfer to PCA 
pca = PCA(n_components = 28)
pca = pca.fit(X_scale)

X_train_pca = pca.transform(X_train_scale)
X_test_pca = pca.transform(X_test_scale)

# step 1: Preparing data for train set and test set

#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)

# step 2. Trainning model by using all independent variable
clf = LinearRegression()
clf.fit(X_train_pca,y_train)

# step 3. Testing model 
y_pred = clf.predict(X_test_pca)

# step 4: evaluate model 
regression_score = clf.score(X_test_pca,y_test)
regression_score

"""### 2.3.3 Ridge Model Training and Performance Analysis

**Conclusion:** 
1. By training the ridge regression model with different alpha values and comparing the training accuracy of different alpha values, we find the best ridge regression model.
2. The best ridge regression model will have $\alpha = 1e-7$. 
3. By using this model, accuracy on the training dataset will be 0.5440.
4. By using this model, accuracy on the testing dataset will be around 0.5428.

"""

#import data for Regulazation Model
y_batch = y.to_numpy()
y_batch = y_batch[:, None]

sc = MinMaxScaler()
X_batch=X_scale

print("Features shape {}".format(X_batch.shape))
print("Label shape {}".format(y_batch.shape))

def train_linear_model_with_diff_params(model_type, alpha_values, X_scaled, y, test_size, model_list= None, train_accuracy_list= None, test_accuracy_list=None):
  # initialize the result 
  if train_accuracy_list == None:
    train_accuracy_list = [];
  if test_accuracy_list == None:
    test_accuracy_list = [];

  if model_list == None:
    model_list = [];

  for spec_alpha in alpha_values: 
    # split the training and testing dataset 
    (X_train, X_test, y_train, y_test) = train_test_split(X_scaled, y, test_size= 0.8, random_state=42)
    if model_type == 'Ridge':
      model = Ridge(alpha=spec_alpha, max_iter= 100000);
    if model_type == 'Lasso':
      model = Lasso(alpha= spec_alpha, max_iter= 100000);
    
    model.fit(X_train, y_train);
    model_list.append(model); 
    train_accuracy = model.score(X_train, y_train);
    test_accuracy = model.score(X_test, y_test); 
    train_accuracy_list.append(train_accuracy);
    test_accuracy_list.append(test_accuracy);
    # format the result for better result display 
    spec_alpha = format(spec_alpha, '.6f')
    train_accuracy = format(train_accuracy, '.13f')
    test_accuracy = format(test_accuracy, '.13f')
    print("{} Model with alpha {} Performance || Train_accuracy:{}, Test_accuracy:{}".format(model_type, spec_alpha, train_accuracy, test_accuracy));
  return model_list, train_accuracy_list, test_accuracy_list;

alpha_values_list = [5,4,3,2,1,0.5, 0.1, 0.05, 0.01, 0.005, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5,
                     5e-6, 1e-6, 5e-7, 1e-7];

ridge_model_list, ridge_train_accur_list, ridge_test_accur_list = train_linear_model_with_diff_params(model_type='Ridge', 
                                                                                                        alpha_values= alpha_values_list, 
                                                                                                        X_scaled= X_batch, 
                                                                                                        y= y_batch, 
                                                                                                        test_size = 0.2)

"""###2.3.4 Lasso Model Training and Performance Analysis

 **Conclusion:** 
1. By training the lasso regression model with different alpha values and comparing the training accuracy of different alpha values, we find the best lasso regression model.
2. The best lasso regression model will have $\alpha = 0.0005$. 
3. By using this model, accuracy on the training dataset will be around 0.5413.  
4. By using this model, accuracy on the testing dataset will be around 0.5402.
"""

alpha_values_list = [5,4,3,2,1,0.5, 0.1, 0.05, 0.01, 0.005, 1e-3, 5e-4];
lasso_model_list, lasso_train_accur_list, lasso_test_accur_list = train_linear_model_with_diff_params(model_type='Lasso', 
                                                                                                        alpha_values= alpha_values_list, 
                                                                                                        X_scaled= X_batch, 
                                                                                                        y= y_batch, 
                                                                                                        test_size = 0.2)

"""## 2.4 Logistic Regression Model

We use PCA to reduce dimensionality of large data sets and train Logistic Regression Model.

### 2.4.1 Apply PCA in Logistic Regression Model
"""

# PCA 

# Preparing data for train set and test set
X = wine_df.drop(columns=['id', 'price', 'wine_class'])
y = wine_df['wine_class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Intermediate step to address scale-invariance
x_train_scale =  StandardScaler().fit_transform(X_train)
x_test_scale =  StandardScaler().fit_transform(X_test)

# Instantiate and Fit PCA
pca = PCA(n_components = 45)
x_train_pca = pca.fit_transform(x_train_scale)
x_test_pca = pca.transform(x_test_scale)

#  Save the explained variance ratios into variable called "explained_variance_ratios"
np.set_printoptions(suppress=True)
explained_variance_ratios = pca.explained_variance_ratio_

# Save the CUMULATIVE explained variance ratios into variable called "cum_evr"
cum_evr = np.cumsum(explained_variance_ratios)

# Plot
plt.figure(figsize=(12,6))
plt.plot(cum_evr)
# Aesthetics 
plt.title('PCA Analysis')
plt.xlabel('Components') 
plt.ylabel('Explained Variance Ratio')
plt.axhline(0.8)
plt.xlim(1,46)  # x range
x=range(1,46,1) 
plt.xticks(x)    # setting for xticks
plt.show()

"""We reduce dimensionality to 28 components."""

# Final PCA: reduce dimensionality to 28 components

# 1: Preparing data for train set and test set
X = wine_df.drop(columns=['id', 'price', 'wine_class'])
y = wine_df['wine_class']

# 2. scale training and testing data
scaler= StandardScaler().fit(X)
X_scale = scaler.transform(X)
X_train_scale, X_test_scale, y_train, y_test = train_test_split(X_scale, y, test_size=0.2, random_state=42)

# 3. transfer to PCA 
pca = PCA(n_components = 28)
pca = pca.fit(X_scale)

X_train_pca = pca.transform(X_train_scale)
X_test_pca = pca.transform(X_test_scale)

"""We train the logistic regression model with Ridge regularization by using PCA data"""

# Training the logistic regression model with Ridge regularization by using PCA data
log_reg_pca = LogisticRegression(penalty='l2', random_state=42, max_iter=5000)
log_reg_pca.fit(X_train_pca, y_train)

#  Use the model to predict on the PCA transformed test set and save these predictions as `y_pred`
y_pred_pca = log_reg_pca.predict(X_test_pca)

# TO-DO: Find the accuracy and store the value in `test_accuracy`
log_reg_score = log_reg_pca.score(X_test_pca, y_test)
print('logistic regression score: {}'.format(log_reg_score))
log_reg_accuracy = accuracy_score(y_test, y_pred_pca)
print('accuracy score: {}'.format(log_reg_accuracy ))

print('Precision: %.3f' %precision_score(y_true=y_test, y_pred=y_pred_pca, labels=[0,1,2,3,4,5,6,7],average='micro'))
print('Recall: %.3f' %recall_score(y_true=y_test, y_pred=y_pred_pca, labels=[0,1,2,3,4,5,6,7],average='micro'))
print('F1: %.3f' %f1_score(y_true=y_test, y_pred=y_pred_pca, labels=[0,1,2,3,4,5,6,7],average='micro'))

"""### 2.4.2 Receiver Operator Characteristics (ROC) and Area Under the Curve (AUC)"""

import numpy as np
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc

y_score = log_reg_pca.fit(X_train_pca, y_train).decision_function(X_test_pca)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()

y_test_2 = label_binarize(y_test, classes=[0,1,2,3,4,5,6,7])

n_classes = y_test_2.shape[1]

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_2[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_2.ravel(), y_score.ravel(), pos_label=1)
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

plt.figure()
lw = 1
plt.style.use('seaborn')
plt.plot(fpr[lw], tpr[lw], color='darkorange',
         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[lw])
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Class %d'%lw)
plt.legend(loc="lower right")
plt.show()

"""### 2.4.3 Learning Curve

From the plot of the learning curve we can see that the gap between the training accuracy and the validation accuracy start to converge at a point when the training size reach 70%.
"""

from sklearn.model_selection import learning_curve

train_sizes, train_scores, test_scores = \
  learning_curve(estimator=log_reg_pca, X=X_scale,
                 y=y,
                 train_sizes=[0.3, 0.5, 0.6, 0.7, 0.8, 0.85],
                 cv=10)
  
plt.plot(train_sizes,np.mean(train_scores, axis=1), \
          marker='o', label='Training accuracy')
plt.plot(train_sizes,np.mean(test_scores, axis=1), \
          marker='+', label='Validation accuracy')
plt.xlabel('Training samples')
plt.legend()

"""### 2.4.4 Validation Curve

We find that the validation curve reaches the optimal point with the regularization parameter C of 0.01, which means the data needs limited regularization strength. This actually matches with the above fact that the accuracy with or without regularization almost stays the same.
"""

from sklearn.model_selection import validation_curve
param_range = [0.001, 0.01, 0.01, 1.0, 5.0, 10.0, 15.0, 20.0]

train_scores, test_scores = validation_curve(estimator=log_reg_pca, X=X_scale,
                                             y=y, param_name='C',
                                             param_range = param_range,
                                             cv=10)

train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)
plt.figure(dpi=100)
plt.plot(param_range, train_mean, color='blue', marker='o',
         markersize=6, label='Training accuracy')
plt.fill_between(param_range, train_mean + train_std, train_mean - train_std, \
                 alpha=0.15)
plt.plot(param_range, test_mean, color='green', marker='+',
         markersize=6, label='Test accuracy')
plt.fill_between(param_range, test_mean + test_std, test_mean - test_std, \
                 alpha=0.15)
plt.xscale('log')
plt.legend(loc='lower left')
plt.ylabel('Accuracy')
plt.xlabel('Regularization parameter C')
plt.show()

"""# PART 3: Random Forest and Neural Network Model

## 3.1 Preparation

### 3.1.1 Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# Data Wrangling
import pandas as pd
import numpy as np

# torch nn
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
from torch.utils.data import DataLoader, TensorDataset
from torch.autograd import Variable
# from collections import Counter

# sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier

# decision tree
from io import StringIO
from sklearn.tree import export_graphviz
from sklearn import tree
from IPython.display import Image  
import pydotplus
import graphviz

#Plotting
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

"""### 3.1.2 Data Preprocessing


"""

wine_df.head(10)

X = wine_df.drop(columns=['id', 'price', 'wine_class'])
y = wine_df['wine_class']
X_batch = X.to_numpy()
y_batch = y.to_numpy()
y_batch = y_batch[:, None]
#min max scale the features
sc = MinMaxScaler()
X_batch=sc.fit_transform(X_batch)

print("Features shape {}".format(X_batch.shape))
print("Label shape {}".format(y_batch.shape))

"""Convert train and test data to tensors in order to create tensor datasets and dataloaders."""

X_train, X_test, y_train, y_test = train_test_split(X_batch, y_batch, test_size=0.2, random_state=42)
#convert to tensors
X_train = torch.from_numpy(X_train)
X_train = X_train.to(torch.float32)
y_train = torch.from_numpy(y_train)
y_train = y_train.to(torch.float32)
X_test = torch.from_numpy(X_test)
X_test = X_test.to(torch.float32)
y_test = torch.from_numpy(y_test)
y_test = y_test.to(torch.float32)
# check the training tensor and testing tensor size 
print("Training Data Tensor Size: features x:{},  label y:{}".format(X_train.shape, y_train.shape))
print("Testing Data Tensor Size: features x:{},  label y:{}".format(X_test.shape, y_test.shape))

"""## 3.2 Forward Neural Network (FNN)

Construct our neural network model. 
Our neural network structure is as follows:

*   one input linear layer
*   one relu layer
*   one hidden layer
*   one relu layer
*   one output linear layer

Create tensor datasets and data loaders.
"""

# batch_size
batch_size = 400

# Pytorch train and test sets
train = TensorDataset(X_train,y_train)
test = TensorDataset(X_test,y_test)


# data loader
train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)
test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)

class FNN(nn.Module):
    def __init__(self):
        super().__init__()
        # TODO
        self.flatten = nn.Flatten() 
        self.relu = nn.ReLU()

       #hidden and output layers
        self.input = nn.Linear(45, 20) 
        self.hidden = nn.Linear(20, 15) 
        self.output = nn.Linear(15, 1) #number of classes

    def forward(self, x):
        # TODO
        outputs = nn.Sequential(self.flatten,self.input, self.relu, self.hidden, self.relu, self.output)(x) #similar with pipline     
        # END TODO
        return outputs

torch.manual_seed(42) 
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(torch.__version__)
print(device)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Sending the data to device (CPU or GPU)
# # TODO
# fnn = FNN().to(device)
# 
# criterion = nn.MSELoss()
# # END TODO 
# # optimizer = optim.SGD(fnn.parameters(), lr=1e-4) #lr - learning step(1e-4, 0.1, etc)
# optimizer = optim.Adam(fnn.parameters(), lr=1e-2) #lr - learning step
# epoch = 16
# acc_LIST_FNN = []
# loss_LIST_FNN = []
# 
# # Train the FNN
# for epoch in range(epoch):
#   running_loss = 0.0
#   correct = 0
#   total = 0
#   for inputs, labels in train_loader:
#       labels = labels.type(torch.FloatTensor) # Cast to Float
#       inputs, labels = inputs.to(device), labels.to(device)
#       # inputs, labels = inputs.to(torch.float32).to(device), labels.to(torch.float32).to(device)
#       # print(inputs)
#       # print(labels)
#       # TODO
#       outputs = fnn(inputs) # Feed the network the train data
#       # print(labels)
#       # print(outputs)
#       # _, preds = torch.max(outputs.data, 1)
#       total += labels.size(0)
#       correct += (outputs.to(torch.int) == labels.to(torch.int)).sum().item()
#       optimizer.zero_grad() # We need to reset the optimizer tensor gradient every mini-batch
#       loss = criterion(outputs, labels) # this is the average loss for one mini-batch of inputs
#       loss.backward() # Do a back propagation
#       optimizer.step()
#       running_loss += loss.item()
#       # print(running_loss)
#       # print(correct)
#       
#   accuracy = 100 * correct / total# Calculate Training Acc
#   acc_LIST_FNN.append(accuracy)
#   loss_LIST_FNN.append(running_loss / len(train_loader) ) # get the avg loss for each epoch
#   
#   # END TODO 
# 
#   # print statistics
#   print("The loss for Epoch {} is: {}, Accuracy = {}%, correct: {}, total: {}".format(epoch+1, running_loss/len(train_loader), accuracy, correct, total))

# TODO
epoch = 16
fnn_acc_vs_epoch_data = {'epoch number':[e for e in range(1,epoch+1)], 'training accuracy':acc_LIST_FNN}
fnn_acc_vs_epoch_df = pd.DataFrame.from_dict(data=fnn_acc_vs_epoch_data)
fig, ax = plt.subplots(figsize=(6,4))
ax_train = sns.lineplot(data = fnn_acc_vs_epoch_df, x = 'epoch number', y = 'training accuracy')
ax_train.set(title = 'Training Accuracy vs Epochs FNN')
# END TODO

total = 0
correct = 0
with torch.no_grad():
    for inputs, labels in test_loader:
        labels = labels.type(torch.FloatTensor) # Cast to Float
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = fnn(inputs) # Feed the network the train data
        total += labels.size(0)
        correct += (outputs.to(torch.int) == labels.to(torch.int)).sum().item()

test_acc_FNN = pd.Series([100 * correct / total], copy = False)
# TODO END
print('Test Accuracy: ' + str(test_acc_FNN.item()) +'%')

"""## 3.3 Deeper Neural Network

Construct our neural network model. 
Our neural network structure is as follows:

*   one input linear layer
*   one relu layer
*   ... (9 hidden layers)
*   one relu layer
*   one output linear layer

From the result we can see that the results of the two NN models generally match.
"""

# batch_size
batch_size = 400

# Pytorch train and test sets
train = TensorDataset(X_train,y_train)
test = TensorDataset(X_test,y_test)


# data loader
train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)
test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)

class DNN(nn.Module):
    def __init__(self):
        super().__init__()
        # TODO
        self.deep_stack = nn.Sequential(
            nn.Flatten(),
            nn.Linear(45,45),
            nn.ReLU(),
            nn.Linear(45,45),
            nn.ReLU(),
            nn.Linear(45,45),
            nn.ReLU(),
            nn.Linear(45,45),
            nn.ReLU(),
            nn.Linear(45,45),
            nn.ReLU(),
            nn.Linear(45,40),
            nn.ReLU(),
            nn.Linear(40,32),
            nn.ReLU(),
            nn.Linear(32,16),
            nn.ReLU(),
            nn.Linear(16,8),
            nn.ReLU(),
            nn.Linear(8,4),
            nn.ReLU(),
            nn.Linear(4,1),
        )
        # END TODO

    def forward(self, x):
        # TODO
        outputs = self.deep_stack(x)
        # END TODO
        return outputs

torch.manual_seed(42) 
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(torch.__version__)
print(device)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Sending the data to device (CPU or GPU)
# # TODO
# dnn = DNN().to(device)
# 
# criterion = nn.MSELoss()
# # END TODO 
# # optimizer = optim.SGD(dnn.parameters(), lr=1e-4) #lr - learning step(1e-4, 0.1, etc)
# optimizer = optim.Adam(dnn.parameters(), lr=1e-2) #lr - learning step
# epoch = 25
# acc_LIST_DNN = []
# loss_LIST_DNN = []
# 
# # Train the DNN
# for epoch in range(epoch):
#   running_loss = 0.0
#   correct = 0
#   total = 0
#   for inputs, labels in train_loader:
#       labels = labels.type(torch.FloatTensor) # Cast to Long
#       inputs, labels = inputs.to(device), labels.to(device)
#       # TODO
#       outputs = dnn(inputs) # Feed the network the train data
#       total += labels.size(0)
#       correct += (outputs.to(torch.int) == labels.to(torch.int)).sum().item()
#       optimizer.zero_grad() # We need to reset the optimizer tensor gradient every mini-batch
#       loss = criterion(outputs, labels) # this is the average loss for one mini-batch of inputs
#       loss.backward() # Do a back propagation
#       optimizer.step()
#       running_loss += loss.item()
#       
#   accuracy = 100 * correct / total# Calculate Training Acc
#   acc_LIST_DNN.append(accuracy)
#   loss_LIST_DNN.append(running_loss / len(train_loader) ) # get the avg loss for each epoch
#   
#   # END TODO 
# 
#   # print statistics
#   # if epoch%10 == 0:
#   print("The loss for Epoch {} is: {}, Accuracy = {}%, correct: {}, total: {}".format(epoch+1, running_loss/len(train_loader), accuracy, correct, total))

# TODO
epoch = 25
dnn_acc_vs_epoch_data = {'epoch number':[e for e in range(1,epoch+1)], 'training accuracy':acc_LIST_DNN}
dnn_acc_vs_epoch_df = pd.DataFrame.from_dict(data=dnn_acc_vs_epoch_data)
fig, ax = plt.subplots(figsize=(6,4))
ax_train = sns.lineplot(data = dnn_acc_vs_epoch_df, x = 'epoch number', y = 'training accuracy')
ax_train.set(title = 'Training Accuracy vs Epochs FNN')
# END TODO

total = 0
correct = 0
with torch.no_grad():
    for inputs, labels in test_loader:
        labels = labels.type(torch.FloatTensor) # Cast to Float
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = dnn(inputs) # Feed the network the train data
        total += labels.size(0)
        correct += (outputs.to(torch.int) == labels.to(torch.int)).sum().item()

test_acc_DNN = pd.Series([100 * correct / total], copy = False)
# TODO END
print('Test Accuracy: ' + str(test_acc_DNN.item()) +'%')

"""## 3.4 Decision Tree

"""

wine_df

X = wine_df.drop(columns=['price', 'wine_class'])
y = wine_df['wine_class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)
#Fit Model
clf = tree.DecisionTreeClassifier(max_depth = 30,random_state = 1) #If depth is not set, it constructs a very deep tree
clf = clf.fit(X_train, y_train)

# Test Model
print('Training Accuracy = ',clf.score(X_train,y_train)) # training data
print('Testing Accuracy = ',clf.score(X_test,y_test)) # testing data

# dot_data = tree.export_graphviz(clf, out_file=None)  
# graph = pydotplus.graph_from_dot_data(dot_data)  
# Image(graph.create_png())

"""##3.5 Random Forests

* Training the Random Forest with 30 delpth, it is able to get the highest accuracy score. 
* However, this model has low testing accuracy.
* The Random Forest Model also has the overfiting problem.
"""

#Fit Model
X = wine_df.drop(columns=['price', 'wine_class'])
y = wine_df['wine_class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)
clf = RandomForestClassifier(max_depth = 30)
clf.fit(X_train,y_train)
prediction = clf.predict(X_test)

# Test Model
print('Training Accuracy = ',clf.score(X_train,y_train)) # training data
print('Testing Accuracy = ',clf.score(X_test,y_test)) # testing data

"""# Model Comparison and Conclusion
* After tuning the parameters, Linear Regression Model performance of Lasso/Ridge regression model is improved. The best performance of Lasso/Ridge model will correspond to accuracy around 54%. 
* Logistic Regression Model: the accuracy after applying with PCA remains stable around 32%.
* Neural network model: apply a neural network with more complex function and more parameters to build a multiclass classifier model.
* Random Forest performs better than Decision Tree in our Model, as it prevents overfitting and provides more accuracy prediction.

# Challenges and Obstacles Faced

* Data selection bias makes this study challenging to extract more determined factors of marketing price in the wine industry.
* At first we wanted to include the province, region, and designation(vineyard) as features. But we found there are too many provinces, regions and designations. If we apply one-hot encoding to all of them, the operation will be at a very high cost. If we select only a portion of them (say, top 20), we will lose too many varieties, and therefore lose too much data for training and prediction. So we decided to give them up as features.
* Originally we would like "winery" to be one of the features. But there are too many wineries to do one hot encoding. And there will not be enough data if we only keep a few wineries (say, top 20). So we calculate the average points for each winery. We decided to use "winery_point" as one feature to represent the quality/reputation of a winery.
* When we first trained a neural network, previously we found that while the loss kept falling, the accuracy wasn't changing. We found during debugging that the predicted output tensor didn't match with the true label tensor because the datatype is float. So we have to cast the tensor datatype from float to int. And finally we could calculate the accuracy correctly.

# Potential Next Steps

* We could combine the data with other dataset to add more valuable features for our prediction. For example the condition of soil/climate for vineyard growing grapes.
* Although we use country instead of region as a feature because most of the wine data come from the top 20 countries that we have selected from the country data. The data is imbalanced as for example wine from the US contribute to around 50% of all wine. We could oversample or undersample the data for balance purpose, meanwhile think of how to balance data without affecting the accuracy of prediction. 
* We use sentiment scores as one of the features for our prediction. But the score calculated by nltk toolkit may not represent the true positivity/negativity of the sentences. We could use other methods to help make the sentiment score more precise for the positivity/negativity of the sentences, thus make the feature more helpful to the prediction.* We could try a more robust and complex neural network, with more hidden layers and varying activation functions.
* Hyperparameter finer tuning will help us to further improve the performance of learning and prediction.
"""